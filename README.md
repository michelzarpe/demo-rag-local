# RAG Local Demo

Este repositÃ³rio foi criado [nesse vÃ­deo](https://youtu.be/j0CdfesdRZQ) e rodando **100% local**, com foco em **arquitetura, dados e decisÃµes de design**, nÃ£o apenas em frameworks.

A ideia Ã© mostrar como uma aplicaÃ§Ã£o RAG funciona **de ponta a ponta**:
- IngestÃ£o de documentos
- Busca semÃ¢ntica
- Uso de contexto
- GeraÃ§Ã£o de respostas com LLM local

A documentaÃ§Ã£o de referÃªncia pode ser encontrada [aqui](https://giulianabezerra.notion.site/RAG-Local-Demo-2cf49c5a6df18010bbf3e32b1505fc10).

## ğŸ¥ Sobre o vÃ­deo

No vÃ­deo, a construÃ§Ã£o da aplicaÃ§Ã£o segue esta ordem:

1. Arquitetura e componentes
2. Fluxo de dados
3. ImplementaÃ§Ã£o prÃ¡tica
4. DiscussÃ£o dos modos de funcionamento do RAG (restrito vs hÃ­brido)

O cÃ³digo aqui serve como **apoio ao raciocÃ­nio arquitetural** apresentado.

## ğŸ·ï¸ Estrutura de versÃµes (tags)

O repositÃ³rio possui duas versÃµes principais:

### ğŸ”¹ `initial`
VersÃ£o inicial para acompanhar o vÃ­deo passo a passo, contendo:

- Estrutura de pastas
- Arquivos Python vazios ou com TODOs
- DependÃªncias definidas

ğŸ‘‰ Ideal para quem quer **construir junto**.

### ğŸ”¹ `complete`
VersÃ£o final com a aplicaÃ§Ã£o totalmente funcional, contendo:

- IngestÃ£o de documentos
- Banco vetorial
- API de perguntas
- Exemplos de documentos

ğŸ‘‰ Ideal como **referÃªncia** ou para quem quer testar direto.


## ğŸ“ Estrutura do projeto

```text
rag-local/
 â”œâ”€â”€ app/
 â”‚   â”œâ”€â”€ main.py        # API (FastAPI)
 â”‚   â”œâ”€â”€ ingest.py      # IngestÃ£o e indexaÃ§Ã£o
 â”‚   â””â”€â”€ rag.py         # LÃ³gica de RAG
 â”œâ”€â”€ data/
 â”‚   â””â”€â”€ docs/          # Documentos de exemplo (PDF, txt, md)
 â”œâ”€â”€ chroma/            # Banco vetorial (gerado localmente)
 â”œâ”€â”€ requirements.txt
 â””â”€â”€ README.md
```

âš ï¸ A pasta chroma/ nÃ£o deve ser versionada. Ela Ã© criada automaticamente ao rodar a ingestÃ£o.

## âš™ï¸ PrÃ©-requisitos

- Python 3.10+
- Git
- [Ollama](https://ollama.com/download) instalado e rodando localmente:
```sh
# InstruÃ§Ãµes para MAC
brew install ollama
ollama serve
```
- Modelos instalados
```sh
ollama pull mistral
ollama pull nomic-embed-text
```

## ğŸ¤– Modelos utilizados

- LLM local: mistral
- Embeddings: nomic-embed-text

Os modelos sÃ£o gerenciados pelo Ollama.

## ğŸš€ Como executar

1.  Clonar o repositÃ³rio na branch initial
```
git clone https://github.com/ -b initial
cd rag-local-demo
```

2. Criar ambiente virtual
```
python3 -m venv .venv
source .venv/bin/activate
```

3. Instalar dependÃªncias
```
pip install -r requirements.txt
```

4. Rodar a ingestÃ£o
```
python app/ingest.py
```

5. Subir a API
```
uvicorn app.main:app --reload
```

6. Fazer uma pergunta
```
curl -X POST http://localhost:8000/pergunta \
  -H "Content-Type: application/json" \
  -d '{"question": "O que Ã© arquitetura de dados?"}'
```

## ğŸ§  ObservaÃ§Ãµes importantes

- O LLM nÃ£o acessa dados diretamente
- Todo acesso a documentos passa pelo banco vetorial
- O comportamento do RAG depende das decisÃµes arquiteturais
- Frameworks sÃ£o substituÃ­veis, o raciocÃ­nio nÃ£o

## ğŸ“Œ Objetivo educacional
Este projeto nÃ£o Ã© um produto pronto, mas um exemplo didÃ¡tico para discutir:
- Arquitetura de IA
- GovernanÃ§a
- Trade-offs de uso de LLMs
- RAG em ambientes controlados

## ğŸ“œ LicenÃ§a
Uso educacional e demonstrativo.

## Arquitetura
![RAG](RAG.PNG)